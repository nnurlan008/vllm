max_position_embeddings: 131072
model_max_length: None
other keys: {'max_position_embeddings': 131072}

INFO 11-29 09:18:22 [utils.py:253] non-default args: {'seed': None, 'disable_log_stats': True, 'enforce_eager': True, 'model': 'meta-llama/Llama-3.1-8B-Instruct'}
WARNING 11-29 09:18:22 [arg_utils.py:1195] `seed=None` is equivalent to `seed=0` in V1 Engine. You will no longer be allowed to pass `None` in v0.13.
INFO 11-29 09:18:22 [model.py:630] Resolved architecture: LlamaForCausalLM
INFO 11-29 09:18:22 [model.py:1745] Using max model len 131072
INFO 11-29 09:18:23 [scheduler.py:207] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 11-29 09:18:23 [vllm.py:508] Cudagraph is disabled under eager mode
[0;36m(EngineCore_DP0 pid=425783)[0;0m INFO 11-29 09:18:23 [core.py:93] Initializing a V1 LLM engine (v0.11.2.dev211+g0ff70821c) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.1-8B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': None, 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 0, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:24 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is unavaible due to: /home/nnaza008/vllm3/vllm/vllm/vllm_flash_attn/_vllm_fa2_C.abi3.so: undefined symbol: _ZN5flash35run_mha_fwd_splitkv_dispatch_customIN7cutlass10bfloat16_tELi256ELb0EEEvRNS_16Flash_fwd_paramsEP11CUstream_st
[0;36m(EngineCore_DP0 pid=425783)[0;0m INFO 11-29 09:18:24 [parallel_state.py:1217] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.128.15.205:37435 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[0;36m(EngineCore_DP0 pid=425783)[0;0m INFO 11-29 09:18:25 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=425783)[0;0m INFO 11-29 09:18:25 [gpu_model_runner.py:3259] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
[0;36m(EngineCore_DP0 pid=425783)[0;0m INFO 11-29 09:18:26 [cuda.py:369] Using AttentionBackendEnum.TRITON_ATTN backend.
[0;36m(EngineCore_DP0 pid=425783)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=425783)[0;0m Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.34s/it]
[0;36m(EngineCore_DP0 pid=425783)[0;0m Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.27it/s]
[0;36m(EngineCore_DP0 pid=425783)[0;0m Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.04s/it]
[0;36m(EngineCore_DP0 pid=425783)[0;0m Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.18s/it]
[0;36m(EngineCore_DP0 pid=425783)[0;0m Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.12s/it]
[0;36m(EngineCore_DP0 pid=425783)[0;0m 
[0;36m(EngineCore_DP0 pid=425783)[0;0m INFO 11-29 09:18:31 [default_loader.py:308] Loading weights took 4.57 seconds
[0;36m(EngineCore_DP0 pid=425783)[0;0m INFO 11-29 09:18:31 [gpu_model_runner.py:3341] Model loading took 14.9889 GiB memory and 5.216400 seconds
[0;36m(EngineCore_DP0 pid=425783)[0;0m INFO 11-29 09:18:33 [gpu_worker.py:348] Available KV cache memory: 19.19 GiB
[0;36m(EngineCore_DP0 pid=425783)[0;0m INFO 11-29 09:18:33 [kv_cache_utils.py:1234] GPU KV cache size: 157,216 tokens
[0;36m(EngineCore_DP0 pid=425783)[0;0m INFO 11-29 09:18:33 [kv_cache_utils.py:1239] Maximum concurrency for 131,072 tokens per request: 1.20x
[0;36m(EngineCore_DP0 pid=425783)[0;0m INFO 11-29 09:18:33 [core.py:253] init engine (profile, create kv cache, warmup model) took 2.21 seconds
[0;36m(EngineCore_DP0 pid=425783)[0;0m INFO 11-29 09:18:34 [vllm.py:508] Cudagraph is disabled under eager mode
INFO 11-29 09:18:35 [llm.py:351] Supported tasks: ['generate']
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 396.59it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][0;36m(EngineCore_DP0 pid=425783)[0;0m BLOCK_M: 16
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [dump_input.py:72] Dumping input data for V1 LLM engine (v0.11.2.dev211+g0ff70821c) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.1-8B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': None, 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 0, 'local_cache_dir': None}, 
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [dump_input.py:79] Dumping scheduler output for model execution: SchedulerOutput(scheduled_new_reqs=[], scheduled_cached_reqs=CachedRequestData(req_ids=['0'], resumed_req_ids=[], new_token_ids=[], all_token_ids={}, new_block_ids=[null], num_computed_tokens=[7], num_output_tokens=[1]), num_scheduled_tokens={0: 1}, total_num_scheduled_tokens=1, scheduled_spec_decode_tokens={}, scheduled_encoder_inputs={}, num_common_prefix_blocks=[1], finished_req_ids=[], free_encoder_mm_hashes=[], preempted_req_ids=[], pending_structured_output_tokens=false, kv_connector_metadata=null, ec_connector_metadata=null)
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844] EngineCore encountered a fatal error.
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]   File "/home/nnaza008/vllm3/vllm/.venv/lib/python3.12/site-packages/triton/language/core.py", line 43, in wrapper
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]   File "/home/nnaza008/vllm3/vllm/.venv/lib/python3.12/site-packages/triton/language/core.py", line 2192, in store
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]     return _semantic.store(pointer, value, mask, boundary_check, cache_modifier, eviction_policy)
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]   File "/home/nnaza008/vllm3/vllm/.venv/lib/python3.12/site-packages/triton/language/semantic.py", line 1297, in store
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]     return self._store_legacy(ptr, val, mask, boundary_check, cache, eviction)
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]   File "/home/nnaza008/vllm3/vllm/.venv/lib/python3.12/site-packages/triton/language/semantic.py", line 1259, in _store_legacy
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]     val = self.broadcast_impl_shape(val, ptr.type.get_block_shapes())
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]   File "/home/nnaza008/vllm3/vllm/.venv/lib/python3.12/site-packages/triton/language/semantic.py", line 705, in broadcast_impl_shape
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]     raise ValueError(f"Cannot broadcast, the expanded size of the tensor ({shape[i]})"
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844] ValueError: Cannot broadcast, the expanded size of the tensor (1) must match the existing size (16) at non-singleton dimension 1: ['16', '16'], ['16', '1']
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844] 
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844] The above exception was the direct cause of the following exception:
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844] 
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]   File "/home/nnaza008/vllm3/vllm/vllm/v1/engine/core.py", line 835, in run_engine_core
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]     engine_core.run_busy_loop()
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]   File "/home/nnaza008/vllm3/vllm/vllm/v1/engine/core.py", line 862, in run_busy_loop
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]     self._process_engine_step()
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]   File "/home/nnaza008/vllm3/vllm/vllm/v1/engine/core.py", line 891, in _process_engine_step
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]     outputs, model_executed = self.step_fn()
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]                               ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]   File "/home/nnaza008/vllm3/vllm/vllm/v1/engine/core.py", line 345, in step
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]     model_output = future.result()
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]                    ^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]   File "/home/nnaza008/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py", line 449, in result
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]     return self.__get_result()
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]   File "/home/nnaza008/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]     raise self._exception
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]   File "/home/nnaza008/vllm3/vllm/vllm/v1/executor/uniproc_executor.py", line 79, in collective_rpc
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]   File "/home/nnaza008/vllm3/vllm/vllm/v1/serial_utils.py", line 479, in run_method
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]   File "/home/nnaza008/vllm3/vllm/vllm/v1/worker/worker_base.py", line 369, in execute_model
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]     return self.worker.execute_model(scheduler_output, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]   File "/home/nnaza008/vllm3/vllm/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]   File "/home/nnaza008/vllm3/vllm/vllm/v1/worker/gpu_worker.py", line 552, in execute_model
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]     output = self.model_runner.execute_model(
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]   File "/home/nnaza008/vllm3/vllm/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]   File "/home/nnaza008/vllm3/vllm/vllm/v1/worker/gpu_model_runner.py", line 2795, in execute_model
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]     model_output = self._model_forward(
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]                    ^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]   File "/home/nnaza008/vllm3/vllm/vllm/v1/worker/gpu_model_runner.py", line 2617, in _model_forward
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]     return self.model(
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]            ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]   File "/home/nnaza008/vllm3/vllm/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]   File "/home/nnaza008/vllm3/vllm/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]   File "/home/nnaza008/vllm3/vllm/vllm/model_executor/models/llama.py", line 617, in forward
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]     model_output = self.model(
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]                    ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]   File "/home/nnaza008/vllm3/vllm/vllm/compilation/decorators.py", line 335, in __call__
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]     return self.forward(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]   File "/home/nnaza008/vllm3/vllm/vllm/model_executor/models/llama.py", line 429, in forward
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]     hidden_states, residual = layer(positions, hidden_states, residual)
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]   File "/home/nnaza008/vllm3/vllm/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]   File "/home/nnaza008/vllm3/vllm/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]   File "/home/nnaza008/vllm3/vllm/vllm/model_executor/models/llama.py", line 345, in forward
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]   File "/home/nnaza008/vllm3/vllm/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]   File "/home/nnaza008/vllm3/vllm/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]   File "/home/nnaza008/vllm3/vllm/vllm/model_executor/models/llama.py", line 247, in forward
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]     attn_output = self.attn(q, k, v)
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]                   ^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]   File "/home/nnaza008/vllm3/vllm/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]   File "/home/nnaza008/vllm3/vllm/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]   File "/home/nnaza008/vllm3/vllm/vllm/attention/layer.py", line 392, in forward
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]     torch.ops.vllm.unified_attention_with_output(
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]   File "/home/nnaza008/vllm3/vllm/.venv/lib/python3.12/site-packages/torch/_ops.py", line 1255, in __call__
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]   File "/home/nnaza008/vllm3/vllm/vllm/attention/utils/kv_transfer_utils.py", line 39, in wrapper
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]   File "/home/nnaza008/vllm3/vllm/vllm/attention/layer.py", line 909, in unified_attention_with_output
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]     self.impl.forward(
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]   File "/home/nnaza008/vllm3/vllm/vllm/v1/attention/backends/triton_attn.py", line 357, in forward
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]     unified_attention_importance(
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]   File "/home/nnaza008/vllm3/vllm/vllm/attention/ops/triton_unified_attention.py", line 1689, in unified_attention_importance
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]     kernel_unified_attention_2d_custom[
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]   File "/home/nnaza008/vllm3/vllm/.venv/lib/python3.12/site-packages/triton/runtime/jit.py", line 419, in <lambda>
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]   File "/home/nnaza008/vllm3/vllm/.venv/lib/python3.12/site-packages/triton/runtime/jit.py", line 733, in run
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]   File "/home/nnaza008/vllm3/vllm/.venv/lib/python3.12/site-packages/triton/runtime/jit.py", line 861, in _do_compile
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]     kernel = self.compile(src, target=target, options=options.__dict__)
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]   File "/home/nnaza008/vllm3/vllm/.venv/lib/python3.12/site-packages/triton/compiler/compiler.py", line 300, in compile
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]     module = src.make_ir(target, options, codegen_fns, module_map, context)
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]   File "/home/nnaza008/vllm3/vllm/.venv/lib/python3.12/site-packages/triton/compiler/compiler.py", line 80, in make_ir
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]     return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844] triton.compiler.errors.CompilationError: at 298:8:
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844] 
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]         output_attn_offset = (
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]             query_offset_0[:, None] * out_attn_stride_1
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]             + query_offset_1[:, None] * out_attn_stride_0
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]             # + seq_offset[None, :] * out_attn_stride_2
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]         )
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844] 
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]         row_mask = query_mask_0[:, None] & query_mask_1[:, None]  # shape (BLOCK_M, 1)
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]         col_mask = tile_mask[None, :]                             # shape (1, TILE_SIZE)
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]         mask = row_mask & col_mask
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844] 
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]         tl.store(
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844]         ^
[0;36m(EngineCore_DP0 pid=425783)[0;0m ERROR 11-29 09:18:36 [core.py:844] Cannot broadcast, the expanded size of the tensor (1) must match the existing size (16) at non-singleton dimension 1: ['16', '16'], ['16', '1']
[0;36m(EngineCore_DP0 pid=425783)[0;0m Process EngineCore_DP0:
Traceback (most recent call last):
  File "/home/nnaza008/vllm3/vllm/t_basic.py", line 58, in <module>
    main()
  File "/home/nnaza008/vllm3/vllm/t_basic.py", line 42, in main
    outputs = llm.generate(prompts, sampling_params)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/nnaza008/vllm3/vllm/vllm/entrypoints/llm.py", line 447, in generate
    outputs = self._run_engine(use_tqdm=use_tqdm)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/nnaza008/vllm3/vllm/vllm/entrypoints/llm.py", line 1737, in _run_engine
    step_outputs = self.llm_engine.step()
                   ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m Traceback (most recent call last):
  File "/home/nnaza008/vllm3/vllm/vllm/v1/engine/llm_engine.py", line 285, in step
    outputs = self.engine_core.get_output()
[0;36m(EngineCore_DP0 pid=425783)[0;0m   File "/home/nnaza008/vllm3/vllm/.venv/lib/python3.12/site-packages/triton/language/core.py", line 43, in wrapper
[0;36m(EngineCore_DP0 pid=425783)[0;0m     return fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=425783)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m   File "/home/nnaza008/vllm3/vllm/.venv/lib/python3.12/site-packages/triton/language/core.py", line 2192, in store
[0;36m(EngineCore_DP0 pid=425783)[0;0m     return _semantic.store(pointer, value, mask, boundary_check, cache_modifier, eviction_policy)
[0;36m(EngineCore_DP0 pid=425783)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m   File "/home/nnaza008/vllm3/vllm/.venv/lib/python3.12/site-packages/triton/language/semantic.py", line 1297, in store
[0;36m(EngineCore_DP0 pid=425783)[0;0m     return self._store_legacy(ptr, val, mask, boundary_check, cache, eviction)
  File "/home/nnaza008/vllm3/vllm/vllm/v1/engine/core_client.py", line 709, in get_output
[0;36m(EngineCore_DP0 pid=425783)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m   File "/home/nnaza008/vllm3/vllm/.venv/lib/python3.12/site-packages/triton/language/semantic.py", line 1259, in _store_legacy
[0;36m(EngineCore_DP0 pid=425783)[0;0m     val = self.broadcast_impl_shape(val, ptr.type.get_block_shapes())
[0;36m(EngineCore_DP0 pid=425783)[0;0m           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m   File "/home/nnaza008/vllm3/vllm/.venv/lib/python3.12/site-packages/triton/language/semantic.py", line 705, in broadcast_impl_shape
[0;36m(EngineCore_DP0 pid=425783)[0;0m     raise ValueError(f"Cannot broadcast, the expanded size of the tensor ({shape[i]})"
[0;36m(EngineCore_DP0 pid=425783)[0;0m ValueError: Cannot broadcast, the expanded size of the tensor (1) must match the existing size (16) at non-singleton dimension 1: ['16', '16'], ['16', '1']
[0;36m(EngineCore_DP0 pid=425783)[0;0m 
[0;36m(EngineCore_DP0 pid=425783)[0;0m The above exception was the direct cause of the following exception:
[0;36m(EngineCore_DP0 pid=425783)[0;0m 
[0;36m(EngineCore_DP0 pid=425783)[0;0m Traceback (most recent call last):
    raise self._format_exception(outputs) from None
vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.
[0;36m(EngineCore_DP0 pid=425783)[0;0m   File "/home/nnaza008/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=425783)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=425783)[0;0m   File "/home/nnaza008/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=425783)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=425783)[0;0m   File "/home/nnaza008/vllm3/vllm/vllm/v1/engine/core.py", line 846, in run_engine_core
[0;36m(EngineCore_DP0 pid=425783)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=425783)[0;0m   File "/home/nnaza008/vllm3/vllm/vllm/v1/engine/core.py", line 835, in run_engine_core
[0;36m(EngineCore_DP0 pid=425783)[0;0m     engine_core.run_busy_loop()
[0;36m(EngineCore_DP0 pid=425783)[0;0m   File "/home/nnaza008/vllm3/vllm/vllm/v1/engine/core.py", line 862, in run_busy_loop
[0;36m(EngineCore_DP0 pid=425783)[0;0m     self._process_engine_step()
[0;36m(EngineCore_DP0 pid=425783)[0;0m   File "/home/nnaza008/vllm3/vllm/vllm/v1/engine/core.py", line 891, in _process_engine_step
[0;36m(EngineCore_DP0 pid=425783)[0;0m     outputs, model_executed = self.step_fn()
[0;36m(EngineCore_DP0 pid=425783)[0;0m                               ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m   File "/home/nnaza008/vllm3/vllm/vllm/v1/engine/core.py", line 345, in step
[0;36m(EngineCore_DP0 pid=425783)[0;0m     model_output = future.result()
[0;36m(EngineCore_DP0 pid=425783)[0;0m                    ^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m   File "/home/nnaza008/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py", line 449, in result
[0;36m(EngineCore_DP0 pid=425783)[0;0m     return self.__get_result()
[0;36m(EngineCore_DP0 pid=425783)[0;0m            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m   File "/home/nnaza008/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
[0;36m(EngineCore_DP0 pid=425783)[0;0m     raise self._exception
[0;36m(EngineCore_DP0 pid=425783)[0;0m   File "/home/nnaza008/vllm3/vllm/vllm/v1/executor/uniproc_executor.py", line 79, in collective_rpc
[0;36m(EngineCore_DP0 pid=425783)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=425783)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m   File "/home/nnaza008/vllm3/vllm/vllm/v1/serial_utils.py", line 479, in run_method
[0;36m(EngineCore_DP0 pid=425783)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=425783)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m   File "/home/nnaza008/vllm3/vllm/vllm/v1/worker/worker_base.py", line 369, in execute_model
[0;36m(EngineCore_DP0 pid=425783)[0;0m     return self.worker.execute_model(scheduler_output, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=425783)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m   File "/home/nnaza008/vllm3/vllm/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=425783)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=425783)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m   File "/home/nnaza008/vllm3/vllm/vllm/v1/worker/gpu_worker.py", line 552, in execute_model
[0;36m(EngineCore_DP0 pid=425783)[0;0m     output = self.model_runner.execute_model(
[0;36m(EngineCore_DP0 pid=425783)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m   File "/home/nnaza008/vllm3/vllm/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=425783)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=425783)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m   File "/home/nnaza008/vllm3/vllm/vllm/v1/worker/gpu_model_runner.py", line 2795, in execute_model
[0;36m(EngineCore_DP0 pid=425783)[0;0m     model_output = self._model_forward(
[0;36m(EngineCore_DP0 pid=425783)[0;0m                    ^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m   File "/home/nnaza008/vllm3/vllm/vllm/v1/worker/gpu_model_runner.py", line 2617, in _model_forward
[0;36m(EngineCore_DP0 pid=425783)[0;0m     return self.model(
[0;36m(EngineCore_DP0 pid=425783)[0;0m            ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m   File "/home/nnaza008/vllm3/vllm/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=425783)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=425783)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m   File "/home/nnaza008/vllm3/vllm/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=425783)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=425783)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m   File "/home/nnaza008/vllm3/vllm/vllm/model_executor/models/llama.py", line 617, in forward
[0;36m(EngineCore_DP0 pid=425783)[0;0m     model_output = self.model(
[0;36m(EngineCore_DP0 pid=425783)[0;0m                    ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m   File "/home/nnaza008/vllm3/vllm/vllm/compilation/decorators.py", line 335, in __call__
[0;36m(EngineCore_DP0 pid=425783)[0;0m     return self.forward(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=425783)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m   File "/home/nnaza008/vllm3/vllm/vllm/model_executor/models/llama.py", line 429, in forward
[0;36m(EngineCore_DP0 pid=425783)[0;0m     hidden_states, residual = layer(positions, hidden_states, residual)
[0;36m(EngineCore_DP0 pid=425783)[0;0m                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m   File "/home/nnaza008/vllm3/vllm/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=425783)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=425783)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m   File "/home/nnaza008/vllm3/vllm/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=425783)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=425783)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m   File "/home/nnaza008/vllm3/vllm/vllm/model_executor/models/llama.py", line 345, in forward
[0;36m(EngineCore_DP0 pid=425783)[0;0m     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
[0;36m(EngineCore_DP0 pid=425783)[0;0m                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m   File "/home/nnaza008/vllm3/vllm/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=425783)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=425783)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m   File "/home/nnaza008/vllm3/vllm/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=425783)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=425783)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m   File "/home/nnaza008/vllm3/vllm/vllm/model_executor/models/llama.py", line 247, in forward
[0;36m(EngineCore_DP0 pid=425783)[0;0m     attn_output = self.attn(q, k, v)
[0;36m(EngineCore_DP0 pid=425783)[0;0m                   ^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m   File "/home/nnaza008/vllm3/vllm/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=425783)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=425783)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m   File "/home/nnaza008/vllm3/vllm/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=425783)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=425783)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m   File "/home/nnaza008/vllm3/vllm/vllm/attention/layer.py", line 392, in forward
[0;36m(EngineCore_DP0 pid=425783)[0;0m     torch.ops.vllm.unified_attention_with_output(
[0;36m(EngineCore_DP0 pid=425783)[0;0m   File "/home/nnaza008/vllm3/vllm/.venv/lib/python3.12/site-packages/torch/_ops.py", line 1255, in __call__
[0;36m(EngineCore_DP0 pid=425783)[0;0m     return self._op(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=425783)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m   File "/home/nnaza008/vllm3/vllm/vllm/attention/utils/kv_transfer_utils.py", line 39, in wrapper
[0;36m(EngineCore_DP0 pid=425783)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=425783)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m   File "/home/nnaza008/vllm3/vllm/vllm/attention/layer.py", line 909, in unified_attention_with_output
[0;36m(EngineCore_DP0 pid=425783)[0;0m     self.impl.forward(
[0;36m(EngineCore_DP0 pid=425783)[0;0m   File "/home/nnaza008/vllm3/vllm/vllm/v1/attention/backends/triton_attn.py", line 357, in forward
[0;36m(EngineCore_DP0 pid=425783)[0;0m     unified_attention_importance(
[0;36m(EngineCore_DP0 pid=425783)[0;0m   File "/home/nnaza008/vllm3/vllm/vllm/attention/ops/triton_unified_attention.py", line 1689, in unified_attention_importance
[0;36m(EngineCore_DP0 pid=425783)[0;0m     kernel_unified_attention_2d_custom[
[0;36m(EngineCore_DP0 pid=425783)[0;0m   File "/home/nnaza008/vllm3/vllm/.venv/lib/python3.12/site-packages/triton/runtime/jit.py", line 419, in <lambda>
[0;36m(EngineCore_DP0 pid=425783)[0;0m     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=425783)[0;0m                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m   File "/home/nnaza008/vllm3/vllm/.venv/lib/python3.12/site-packages/triton/runtime/jit.py", line 733, in run
[0;36m(EngineCore_DP0 pid=425783)[0;0m     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
[0;36m(EngineCore_DP0 pid=425783)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m   File "/home/nnaza008/vllm3/vllm/.venv/lib/python3.12/site-packages/triton/runtime/jit.py", line 861, in _do_compile
[0;36m(EngineCore_DP0 pid=425783)[0;0m     kernel = self.compile(src, target=target, options=options.__dict__)
[0;36m(EngineCore_DP0 pid=425783)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m   File "/home/nnaza008/vllm3/vllm/.venv/lib/python3.12/site-packages/triton/compiler/compiler.py", line 300, in compile
[0;36m(EngineCore_DP0 pid=425783)[0;0m     module = src.make_ir(target, options, codegen_fns, module_map, context)
[0;36m(EngineCore_DP0 pid=425783)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m   File "/home/nnaza008/vllm3/vllm/.venv/lib/python3.12/site-packages/triton/compiler/compiler.py", line 80, in make_ir
[0;36m(EngineCore_DP0 pid=425783)[0;0m     return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,
[0;36m(EngineCore_DP0 pid=425783)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=425783)[0;0m triton.compiler.errors.CompilationError: at 298:8:
[0;36m(EngineCore_DP0 pid=425783)[0;0m 
[0;36m(EngineCore_DP0 pid=425783)[0;0m         output_attn_offset = (
[0;36m(EngineCore_DP0 pid=425783)[0;0m             query_offset_0[:, None] * out_attn_stride_1
[0;36m(EngineCore_DP0 pid=425783)[0;0m             + query_offset_1[:, None] * out_attn_stride_0
[0;36m(EngineCore_DP0 pid=425783)[0;0m             # + seq_offset[None, :] * out_attn_stride_2
[0;36m(EngineCore_DP0 pid=425783)[0;0m         )
[0;36m(EngineCore_DP0 pid=425783)[0;0m 
[0;36m(EngineCore_DP0 pid=425783)[0;0m         row_mask = query_mask_0[:, None] & query_mask_1[:, None]  # shape (BLOCK_M, 1)
[0;36m(EngineCore_DP0 pid=425783)[0;0m         col_mask = tile_mask[None, :]                             # shape (1, TILE_SIZE)
[0;36m(EngineCore_DP0 pid=425783)[0;0m         mask = row_mask & col_mask
[0;36m(EngineCore_DP0 pid=425783)[0;0m 
[0;36m(EngineCore_DP0 pid=425783)[0;0m         tl.store(
[0;36m(EngineCore_DP0 pid=425783)[0;0m         ^
[0;36m(EngineCore_DP0 pid=425783)[0;0m Cannot broadcast, the expanded size of the tensor (1) must match the existing size (16) at non-singleton dimension 1: ['16', '16'], ['16', '1']
Processed prompts:   0%|          | 0/1 [00:01<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
